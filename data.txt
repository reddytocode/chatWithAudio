Course Name: "Wizeline ChatGPT Masterclass"
Module name: 1_nlp_and_genAI Class name: 1.1 AI Overview.mp3
Content: Welcome to this introduction to the magnificent world of artificial intelligence. In this session, we'll chat about AI basics, laying the foundation for the rest of this course. Let's get started. Artificial intelligence, often abbreviated as AI, is a field that can cover two main areas, which are hardware and software. In hardware, it's the field of robotics, and in software, it's the field of machine learning. So when we talk about software, it is a field of computer science that focuses on creating systems capable of performing tasks that typically require human intelligence. These tasks include problem-solving, learning perception, reasoning, understanding natural language, navigation, and more. AI systems aim to replicate or simulate human cognitive abilities and can operate autonomously. AI is important for a number of reasons. AI can automate repetitive and time-consuming tasks, increasing efficiency and productivity in various industries. It can also process and analyze vast amounts of data quickly and accurately, helping organizations make data-driven decisions. AI drives innovation by enabling new technologies, such as self-driving cars, voice assistants, and personalized content recommendations. It's capable of tackling complex problems in fields like healthcare, finance, and climate science, where human capabilities alone fall short. And, by helping automate tasks and improve processes, AI can reduce operational costs. Now, let's chat about the history of AI. The history of AI dates back to ancient myths of automatons and mechanical devices. However, the modern era of AI can be divided into several key milestones. In the 50s, AI as a field was born, with the development of symbolic or good old-fashioned AI, which used rules and symbols to simulate human thought processes. The 70s and 80s are known as the AI winter, a period of reduced funding and progress due to overambitious expectations and limitations of early AI technologies, also limitations in computational power. Then, in the 90s and early 2000s, we saw the emergence of machine learning and neural networks, which led to significant breakthroughs in AI applications, including speech recognition and image classification. The stage where we are today, that began in 2010, is called the AI renaissance. Driven by deep learning, big data, and increased computing power, this era has seen remarkable progress in areas like natural language processing, computer vision, and robotics. Now, let's take a look to the methodologies and processes driving the AI renaissance today. Most current methodologies in AI involve data science. We should remember that if there is no data, there is no AI. Machine learning, deep learning, and reinforcement learning are the most prominent methodologies, where the two later are specialized subsets of the first. Let's review the basics of these methodologies. Machine learning is a subset of AI that focuses on developing algorithms and models that enable computers to learn from and make predictions or decisions based on data. In other words, these algorithms rely on historical data to recognize patterns and make predictions, take actions, or make decisions. It includes supervised learning with labeled data, or when we show the algorithm or the model the right answer, unsupervised learning that has no labels and creates groups of data or clusters, and semi-supervised learning, which is a mix of labeled and unlabeled data, or a mix of supervised and unsupervised learning. Deep learning is a subfield of machine learning that uses artificial neural networks with multiple layers, also called deep neural networks, to model and solve complex problems. Deep neural networks consist of multiple layers of interconnected nodes or neurons, used to automatically learn relevant features from the data, eliminating the need for manual feature engineering. Deep learning excels in tasks such as image and speech recognition, natural language processing, and autonomous systems. Reinforcement learning is a type of machine learning where an agent interacts with an environment and learns to make a sequence of decisions to maximize a cumulative reward. The agent takes actions based on its current state, observes the consequences, also called rewards or penalties, and uses these experiences to learn a policy that maximizes the expected long-term reward. In simple words, it uses the results of what it is trying to do to adjust to achieve the correct answer. Thus, reinforcement learning involves a trade-off between exploring new actions to discover better strategies, and exploiting known actions to maximize immediate rewards. We're seeing AI contribute to economic growth by enhancing productivity and creating new business opportunities. AI is also transforming healthcare, accelerating drug discovery, predicting disease outbreaks, and enabling telemedicine. With self-driving cars and drones, AI has the potential to shake up the transportation industry. AI is also playing a role in climate change mitigation, aiding in climate modeling, renewable energy optimization, and environmental monitoring. And notably, AI is transforming the job market. While AI may lead to job displacement in some industries not confirmed, maybe just evolved, it is also rapidly creating new roles in AI development, data analysis, and more. In conclusion, Artificial Intelligence is a rapidly evolving field with a rich history and wide-ranging applications, making it a crucial technology for the present and the future. Its potential to transform industries and improve human life continues to grow day by day. We're thrilled to have you on board in this journey, I really, really hope you enjoy this journey as much as we enjoyed it, and be ready to learn and harness the potential of AI. Thanks for learning with us.
Module name: 1_nlp_and_genAI Class name: 1.2 Gen AI Intro.mp3
Content: Welcome to this introduction to generative artificial intelligence. In this session, we'll do a deep dive into this subset of AI. So what is generative AI anyway? And why is it important? As a subset of artificial intelligence, generative AI refers to the use of algorithms and models to generate new data, content, or information based on patterns and structures learned from existing data. It creates content such as a human will create it. This technology has seen significant developments over the last 60 years. Let us briefly review its history. Before the 90s, early generative systems like expert systems and rule-based approaches were used for content generation. Then, in the 90s, probabilistic approaches such as Markov models and hidden Markov models were employed for generating sequences of words or sentences. Since then, we've seen a powerful resurgence of generative AI, characterized by the rise of neural networks and deep learning techniques. The methodology behind the generative AI we're seeing today is based on variational autoencoders, generative adversarial networks, and autoregressive models. Variational autoencoders encode data into a latent space, which is a compressed representation of data, and decode it to generate new data samples. Applications of Bayes include image generation, anomaly detection, and data augmentation. Generative adversarial networks consist of a generator and a discriminator, often used in image generation, style transfer, and deep fake creation. Generative models generate data sequentially, where each element is conditioned on the previous elements, such as recurrent neural networks and transformers. These recurrent and transformer models are used for text generation, language translation, and code generation. Looking to the future, the field of generative AI holds many exciting possibilities. Human AI is set to become even more creative and capable of generating content that is increasingly indistinguishable from human creations. Moving forward, some important things to consider include how generative AI could revolutionize interdisciplinary fields such as art, science, and education, how ethical concerns relate to deep fakes, intellectual property, and misinformation can be carefully managed, and how we can ensure responsible development of generative AI using ethical and legal frameworks to ensure a positive impact on society. We should remember AI is an interdisciplinary field. That means it can be used in several areas, and I strongly believe that we should never forget all these aspects that benefit humanity in order to do good and do well.
Module name: 1_nlp_and_genAI Class name: 1.3 NLP.mp3
Content: Welcome to the Fundamentals of Natural Language Processing. Natural Language Processing, also known as NLP, is a field of research that lies at the intersection of computer science, linguistics, and artificial intelligence. It focuses on communication between computers and humans in natural language, with the ultimate goal of enabling computers to understand and generate human language. If you've ever used a voice assistant like Apple's Siri or Amazon's Alexa, you've already interacted with NLP in action. Natural Language Processing is divided into three main subfields, Speech Recognition, Natural Language Understanding, and Natural Language Generation. Each of these subfields is underscored by a difficult challenge, being able to understand and replicate human language and all of its complexity. Human language is a system that involves a syntactic element based on symbols and grammar rules, and a semantic element based on giving meanings to combinations of those symbols. This system is incredibly complex. For example, we can convey the same meaning with different words. There are infinite ways to arrange words in a sentence, and some words can have many different meanings. To demonstrate just how complex human language is, just think of this sentence, I saw a man on a hill with a telescope. This sentence could describe two completely different scenarios, all based on context. Either I could have the telescope and be looking through it, or the man on the hill could have the telescope. Getting a computer to understand complexities like this one is just one example of the challenges that NLP sets out to tackle. Natural Language Processing is used for a wide variety of language-related tasks, including answering questions, classifying text in different ways, and conversing with users. Let's walk through some of the most common tasks that can be solved using NLP. NLP is often used for sentiment analysis, which is a process of classifying the emotional intent of text. In a sentiment classification model, the input is a piece of text, and the output is the probability that the sentiment expressed by that text is positive, negative, or neutral. NLP is behind machine translation, which automates translation between different languages. In machine translation, the input is text in a specified source language, and the output is the text in a specified target language. NLP can also be used for text generation, or natural language generation, in which it produces text that's similar to human-written text. Such models can be fine-tuned to produce text in different genres and formats, including tweets, blogs, and even computer code. The list goes on and on. We're also seeing NLP be used for topic modeling, information retrieval, named entity recognition, summarization, question answering, and many more. Now that we understand what NLP is and does, let's explore how it works. NLP models function by finding relationships between the constituents of language. The letters, the words, and the sentences found in a text dataset. That's a lot of data to process and pre-process. The methods that NLP architectures use to process all this data can roughly be divided into four categories. Rule-based systems, statistical NLP, machine learning, and pre-trained language models. Early NLP systems relied on handcrafted rules and patterns to process language. These rule-based systems were effective for simple tasks, but they struggled with the complexity and variability of natural language. Statistical models use data-driven techniques to analyze and generate language. A big leap from the early rule-based systems, the statistical models brought significant improvement for various NLP tasks. With the advent of machine learning, NLP shifted even further towards more data-driven approaches. Techniques like supervised learning, unsupervised learning, and deep learning, particularly with neural networks, have become standard in the field. Now, pre-trained language models like BERT and GPT have revolutionized NLP. They are pre-trained on vast amounts of text data and fine-tuned for specific tasks, achieving state-of-the-art performance in many NLP applications. The NLP implementations we see today typically belong to the last three categories, statistical NLP, machine learning, and pre-trained language models. With that in mind, let's dive deeper into the preprocessing and feature extraction methods that underlie these three methodologies. Before a model can process text for a specific task, the text often needs to be preprocessed to improve model performance. Putting it simply, preprocessing is putting words and characters into a format that the model can understand. That's accomplished with a couple of key techniques. Stemming and lemmatization convert words to their roots. For instance, educational to education. Sentence segmentation breaks large pieces of text into meaningful sentence units. Stopword removal aims to remove common filler words like this, then, that, not, etc. And tokenization splits text into individual words and word fragments. In addition to data preprocessing, most conventional statistical and machine learning methods also rely on the production of features. Features are numbers that describe a word or document in relation to the corpus that contains it. In order to produce features, we have three different approaches. First, we have bag-of-words, which counts the number of times a word or combination of words appears. Then, we have term-frequency and inverse-document-frequency, or TF-IDF for short. This tool weighs each word by its importance. And finally, we have embeddings, which capture the semantic and contextual meaning of words by positioning them in relation to each other. Embeddings allow NLP models to work with words as vectors, ultimately making it easier for algorithms to understand and process language. With an understanding of how NLP works, let's dive into how NLP is modeled. Most of the NLP tasks we've explored so far can be modeled by a dozen or so general techniques, which fit into two categories, statistical and traditional machine learning methods on one hand, and deep learning methods on the other. We'll start with statistical and traditional machine learning methods, and I'll explain logistic regression, Naive Bayes, decision trees, latent Dirichlet allocation, and hidden Markov models. Logistic regression is a supervised classification algorithm that aims to predict the probability that an event will occur based on some input. In NLP, logistic regression models can be applied to solve problems such as sentiment analysis, spam detection, and toxicity classification. Naive Bayes is a supervised classification algorithm that finds the conditional probability distribution using the Bayes formula. In NLP, this statistical method can be applied to solve problems such as spam detection or finding bugs in software code. Decision trees are a class of supervised classification models that split datasets based on different features to maximize information gain in those splits. In this context, they are used to solve problems such as text classification, sentiment analysis, and named entity recognition. Latent Dirichlet allocation, or LDA for short, is used for topic modeling. As a statistical approach, LDA tries to view a document as a collection of topics and a topic as a collection of words. The intuition behind LDA is that we can describe any topic using only a small set of words from the corpus. Hidden Markov models are probabilistic models that decide the next state of a system based on the current state. For example, in NLP, we might suggest the next word based on the previous word. Then we can use a product of these transition probabilities to find the probability of a whole sentence. Now that we've covered statistical and traditional machine learning methods, let's dive into deep learning methods. I'm going to explain convolutional neural networks, recurrent neural networks, autoencoders, decoders sequence-to-sequence, and transformers. Convolutional neural networks classify text by seeing a document as an image. However, instead of pixels, the input are sentences or documents represented as a matrix of words. Recurrent neural networks remember previous information using hidden states. Then they connect this previous information to the current task to figure out sense in sequences of words. Autoencoders are deep learning encoder-decoders that approximate a mapping from X to X, meaning the input is the same as the output. They first compress the input features into a lower dimensional representation, sometimes called a latent code, latent vector, or latent representation. And then they learn to reconstruct the input. The first process is known as dimensionality reduction. Encoder-decoder sequence-to-sequence architecture is an adaptation of autoencoders specialized for translation, summarization, and similar tasks. The encoder encapsulates the information in a text into an encoded vector. Unlike an autoencoder, instead of reconstructing the input from the encoded vector, the decoder's task is to generate a different desired output, like a translation or a summary. Finally, transformer architecture omits recurrence and instead relies entirely on a self-attention mechanism to draw global dependencies between input and output. This self-attention mechanism is very important. It processes all words at once instead of one at a time. And this process has revolutionized NLP in recent years, namely allowing the birth of chat-GPT. Now, in order to bring all of this to life, you're going to need to familiarize yourself with programming languages and libraries that support NLP. Here's my honorable mentions. We all know Python. Since most libraries and frameworks for deep learning are written for Python, it's the go-to programming language for NLP. The Python libraries most commonly used for NLP are Natural Language Toolkit, or NLTK, spaCy, TensorFlow, and PyTorch. Now, the open-source community known as HuggingFace is also worth mentioning here, since their repository of transformers libraries enable easy customization and training for the models. By understanding how natural language processing works and familiarizing yourself with the tools bringing it to life, the possibilities for innovation are endless. Natural language processing is a dynamic and evolving field with a broad array of applications like search engines, virtual assistants, social media analysis, language translation, spam detection, content generation, and so much more. NLP has revolutionized the way we interact with and benefit from technology, and the biggest example of this is ChatGPT. Understanding NLP is key to being able to innovate with ChatGPT. Looking to the future, as NLP techniques continue to advance, they're likely to play an even more significant role in our daily lives. Thanks for learning with us.
Module name: 1_nlp_and_genAI Class name: 1.4 LLM Intro.mp3
Content: Welcome to this introduction to a fascinating technique in AI known as Large Language Models. In this session, we will be presenting the basic concepts surrounding these very important models. Large Language Models, also known as LLMs, are sophisticated artificial intelligence models designed to understand and generate human language. These models are built using deep learning architectures, leveraging deep neural networks, and using transformer architectures, which have revolutionized natural language processing due to their ability to capture complex patterns and dependencies within textual data. LLMs are trained on massive amounts of text data. They perform various language-related tasks, including text generation, translation, summarization, question answering, sentiment analysis, and many more. Popular Large Language Models have taken the world by storm, being adopted across many different industries. We've obviously all heard of ChatGPT, and this is basically an LLM taking the shape of a generative AI chatbot. Popular LLM models include the following. Google's Pathways Language Model, also known as PALM, built for common sense and arithmetic reasoning, joke explanation, code generation, and translation. Google also created BERT, the Bidirectional Encoder Representations from Transformers, designed to understand natural language and answer questions. We also have ExcelNet, for example, which in comparison to BERT, is able to generate output predictions in random order rather than sequential order. And of course, we have the most well-known LLM, OpenAI's GPT, which is the basis of ChatGPT. GPT stands for Generative Pre-trained Transform, which can be fine-tuned to perform specific tasks downstream. Now that we've got an understanding of what LLMs are, let's chat about how they work. As mentioned before, LLMs rely on transformer architectures. These models are typically pre-trained on large datasets to learn the structure and relationships within the text. During training, the model ingests vast amounts of text data and learns the contextual relationships between words, phrases, and sentences. This process enables the model to predict the next word in a sentence, understand meaning in context, and generate coherent and relevant text. The key to an LLM's success lies in what are known as self-attention mechanisms. These mechanisms enable the model to weigh the significance of different words in a sentence, reflecting on their dependencies and relationships. Let us explore the architecture of LLMs more in detail. The core architecture of LLMs is a transformer model that consists of an encoder and a decoder. As mentioned before, LLMs use a self-attention mechanism where each word or token in a sentence interacts with every other word in that sentence. Such an interaction allows a model to understand the context and dependencies between words, capturing long-range relationships within the text. As for their training, LLMs require vast amounts of text data in this task. This text data is tokenized, then converted into numerical representations, and then divided into chunks or batches. The model is pre-trained on a large corpus of text, learning the statistical relationships between words. This step is unsupervised and helps the model learn the intricacies of language. After pre-training, the model can be fine-tuned for specific tasks by training it on a smaller, task-specific dataset. This means that we adjust the model's parameters to better suit the new task. When given an input, and by that I mean a prompt or a question, the model tokenizes the text. Then, the model converts this tokenization into the numerical format that the model itself understands. Afterwards, the model applies its learned knowledge to understand the context of the input and generate responses to it. When given an input, and by that I mean a prompt or a question, the model tokenizes the text. Then, the model converts this tokenization into the numerical format that the model itself understands. Afterwards, the model applies its learned knowledge to understand the context of the input and generate responses to it. In essence, what the model does is use the self-attention mechanism to weigh the importance of different words in the sequence. Then, it predicts the most probable next word or sequence of words and uses this prediction to generate an answer for the given prompt or question. With a handle on how LLMs work, let's check out how they're being used. We're seeing various practical applications of LLMs. As mentioned before, some of these applications are natural language understanding, content generation, sentiment analysis, code generation, and even medical and scientific research. These applications of LLMs are highly relevant not only in the tech industry, but in almost any field you can imagine. In healthcare, for instance, LLMs are being used to better understand proteins, molecules, DNA, RNA. They're assisting in the development of vaccines, in finding cures for illnesses, and in improving medicines. Moreover, LLMs might allow us to build medical chatbots that would perform patient intakes or basic diagnosis. How about marketing? Here, LLMs are being used to perform sentiment analysis to then see how consumers are responding. They're also being used to inspire and build campaign ideas, pitches, and many marketing strategies. Another example, LLMs are also revolutionizing legal practice. From searching through massive textual datasets to generating law statements, LLMs are increasingly assisting lawyers, paralegals, and legal staff. Looking to the future of LLMs, the focus is on building tailored models for specific industries and tasks. We also want to build multimodal understanding. We want to improve the efficiency and scalability of the models. We want to address bias, address misinformation, address ethical concerns in the generated content. As LLMs evolve, their potential applications will expand, transforming how we interact, generate, and understand human language. This will happen in many domains, so stay tuned to be a part of it.
